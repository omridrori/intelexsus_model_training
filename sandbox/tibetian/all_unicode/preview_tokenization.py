import argparse
from pathlib import Path
from tokenizers import Tokenizer

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--tokenizer", default=str(Path(__file__).parent / "results" / "train_unicode_tokenizer_results" / "tokenizer.json"))
    args = ap.parse_args()
    tk = Tokenizer.from_file(args.tokenizer)

    text1 = """ཤར་ཕྱོགས་རི་བོའི་རྩེ་ནསདཀར་གསལ་ཟླ་བ་ཤར་བྱུངམ་སྐྱེས་ཨ་མའི་ཞལ་རསཡིད་ལ་འཁོར་འཁོར་བྱས་བྱུང་གླུ ན་ནིང་བཏབ་པའི་ལྗང་གཞོནད་ལོ་སོག་མའི་ཕོན་ལྕོགཕོ་གཞོན་རྒས་པའི་ལུས་པོལྷོ་གཞུ་དེ་ལས་གྱོང་བ་གླུ རང་སེམས་སོང་བའི་མི་དེགཏན་གྱི་མདུན་མར་བྱུང་ནརྒྱ་མཚོའི་གཏིང་ནས་ནོར་བུལོན་པ་དེ་དང་མཉམ་བྱུང་གླུ འགྲོ་ཞོར་ལམ་བུའི་སྙིང་ཐུབལུས་དྲི་ཞིམ་པའི་བུ་མོགཡུ་ཆུང་གྲུ་དཀར་བརྙེད་ནསསྐྱུར་བ་དེ་དང་འདྲ་བྱུང་གླུ མི་ཆེན་དཔོན་པོའི་སྲས་མོཁམས་འབྲས་མཚར་ལ་བལྟས་ནཁམ་སྡོང་མཐོན་པོའི་རྩེ་ནསའབྲས་བུ་སྨིན་པ་འདྲ་བྱུང་གླུ སེམས་པ་ཕར་ལ་ཤོར་ནསམཚན་མོའི་གཉིད་ཐེབས་གཅོག་གིཉིན་མོ་ལག་ཏུ་མ་ལོནཡིད་ཐང་ཆད་རོགས་ཡིན་པ་གླུ མེ་ཏོག་ནམ་ཟླ་ཡལ་སོངགཡུ་སྦྲང་སེམས་པ་མ་སྐྱོབྱམས་པའི་ལས་འཕྲོ་ཟད་པརང་ནི་སྐྱོ་རྒུཡ་མི་འདུག་གླུ རྩི་ཐོག་བ་མོའི་ཁ་ལསྐྱ་སེར་རླུང་གི་ཕོ་ཉམེ་ཏོག་སྦྲང་བུ་གཉིས་ཀྱིའབྲལ་མཚམས་བྱེད་མཁན་ལོས་ཡིན་གླུ ངང་པ་འདམ་ལ་ཆགས་ནསརེ་ཞིག་སྡོད་དགོས་བསམས་ཀྱངམཚོ་མོ་དར་ཁ་འགྲིགས་ནསརང་སེམས་ཁོ་ཐག་ཆོད་སོང་གླུ གྲུ་ཤན་སེམས་པ་མེད་ཀྱངརྟ་མགོས་ཕྱི་མིག་བལྟས་བྱུངཁྲེལ་གཞུང་མེད་པའི་བྱམས་པསང་ལ་ཕྱི་མིག་མི་ལྟ་གླུ ང་དང་ཚོང་འདུས་བུ་མོའིཚིག་གསུམ་དམ་བཅའི་མདུད་པཁྲ་བོའི་སྦྲུལ་ལ་མ་བརྒྱབརང་རང་ས་ལ་གྲོལ་སོང་གླུ ཆུང་འདྲིས་བྱམས་པའི་རླུང་བསྐྱེདལྕང་མའི་ལོགས་ལ་བཙུགས་ཡོདལྕང་སྲུང་ཨ་ཇོ་ཞལ་ངོསརྡོ་ཀ་རྒྱག་པ་མ་གནང་གླུ བྲིས་པའི་ཡི་གེ་ནག་ཆུངཆུ་དང་ཐིག་པས་འཇིག་སོངམ་བྲིས་སེམས་
།"""

    text2 = """རྩ་བ་ལྔ་བསྐོལ་བའི་ཁུ་བ་མཁུར་བར་དགང་བ་སྟེ་། ཏིལ་མར་གྱི་སྣ་སྨན་ཡང་སྦྱར་རོ་མཁྲིས་པ་ལས་གྱུར་པའི་ལྐོག་མའི་ནད་ལ་ནི་ཁྲག་ཕྱུང་ལ་། ཁར་དཀར་པོ་དང་། པྲི་ཡང་ཀུ་དང་། སྦྲང་རྩི་སྦྱར་བས་བསྐུ་བར་བྱའོ་སེང་ཕྲོམ་དང་། རྨ་ཤིང་བསྐོལ་བའི་ཁུ་བ་མཁུར་བར་བཅང་ངོ་རྒུན་འབྲུ་དང་། པ་རུ་ཀ་ཤ་བསྐོལ་བའི་ཁུ་བ་མཁུར་བར་བཅངས་ན་ཡང་ཕན་ནོ་ཁྲག་ལས་གྱུར་པའི་ལྐོག་མའི་ནད་ལ་ནི་ཆོ་ག་གོང་མ་བཞིན་དུ་བྱས་ཏེ་གསོ་བར་བྱའོ་ཁྲག་ལས་གྱུར་པའི་ལྐོག་མའི་ནད་ནི་གསོ་དཀའ་སྟེ་སྤང་བར་བྱའོ་ཞེས་ཀྱང་བཤད་དོ་བད་ཀན་ལས་གྱུར་པའི་ལྐོག་མའི་ནད་ལ་ནི་མེ་མདག་དུད་པ་ཅན་གྱིས་བདུག་པ་དང་། སྨན་ཚ་བ་རྣམས་ཀྱིས་གདབ་པར་བྱའོ་ཡང་ན་ཨཨ་པ་མརྒའི་འབྲས་བུ་དང་། གེ་རི་ཀརྣི་དཀར་པོ་དང་། དན་ཏི་དང་། བྱི་དང་ག་དང་། རྒྱམ་ཚ་དང་། དེ་རྣམས་ཏིལ་མར་དང་སྦྱར་བས་སྣ་སྨན་དང་མཁུར་བར་བཅང་པ་རབ་ཏུ་བསྔགས་སོ་ལྐོག་མའི་ནད་བྲིན་ད་དང་། ཤ་ལུ་ཀ་དང་། དུ་དྷི་ཀེ་རི་ཀ་དང་། ཀྱི་ལ་ཡུ་ཀ་དང་། དེ་རྣམས་ལ་ཡང་གསོ་བ་གོང་མ་བཞིན་དུ་བྱའོ་ལྐོག་མའི་འབྲས་ནད་ལ་ནི་ཁྲག་གཟགས་ཏེ་། འབྲས་བུ་གསུམ་དང་། གི་ཝང་དང་། སྐྱེར་ཁནད་དང་། བཙག་ཡུག་སྣམ་དང་། སེང་ཕྲོམ་དང་། ལན་ཚབ༹་དང་། རྨ་ཤིང་དང་། པི་པི་ལིང་དང་། དེ་རྣམས་ཀྱིས་བྱུག་པ་དང་། མཁུར་བར་བཅང་བར
"""

    samples = [("text1", text1), ("text2", text2)]

    out_dir = Path(__file__).parent / "results" / "preview_tokenization_results"
    out_dir.mkdir(parents=True, exist_ok=True)

    out_path = out_dir / "tokens_preview.txt"
    with out_path.open("w", encoding="utf-8") as f:
        for name, t in samples:
            enc = tk.encode(t)
            line = f"{name}: {len(enc.tokens)} tokens\n" + " ".join(enc.tokens) + "\n\n"
            print(line)
            f.write(line)
    print(f"Saved to: {out_path}")

if __name__ == "__main__":
    main()
