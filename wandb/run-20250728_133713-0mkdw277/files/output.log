  0%|                                                                                                                | 0/1232547 [00:00<?, ?it/s]C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\torch\nn\modules\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
  0%|                                                                                                  | 150/1232547 [02:25<173:17:26,  1.98it/s]Traceback (most recent call last):
{'loss': 10.2464, 'grad_norm': 8.630624771118164, 'learning_rate': 4.999983773438254e-05, 'epoch': 0.0}
{'loss': 9.9775, 'grad_norm': 7.397487640380859, 'learning_rate': 4.999963490236073e-05, 'epoch': 0.0}
{'loss': 9.8559, 'grad_norm': 7.2824225425720215, 'learning_rate': 4.9999432070338905e-05, 'epoch': 0.0}
{'loss': 9.7935, 'grad_norm': 5.002936363220215, 'learning_rate': 4.999922923831708e-05, 'epoch': 0.0}
{'loss': 10.772, 'grad_norm': 7.157462120056152, 'learning_rate': 4.9999026406295255e-05, 'epoch': 0.0}
{'loss': 14.4113, 'grad_norm': 7.499056339263916, 'learning_rate': 4.999882357427344e-05, 'epoch': 0.0}
{'loss': 9.3987, 'grad_norm': 7.284510135650635, 'learning_rate': 4.999862074225162e-05, 'epoch': 0.0}
{'loss': 9.4758, 'grad_norm': 6.08780574798584, 'learning_rate': 4.9998417910229796e-05, 'epoch': 0.0}
{'loss': 9.9937, 'grad_norm': 5.41221809387207, 'learning_rate': 4.9998215078207975e-05, 'epoch': 0.0}
{'loss': 9.1389, 'grad_norm': 6.680385112762451, 'learning_rate': 4.999801224618615e-05, 'epoch': 0.0}
{'loss': 8.9775, 'grad_norm': 5.799968242645264, 'learning_rate': 4.999780941416433e-05, 'epoch': 0.0}
{'loss': 8.7347, 'grad_norm': 5.469222068786621, 'learning_rate': 4.999760658214251e-05, 'epoch': 0.0}
{'loss': 8.8375, 'grad_norm': 5.019886016845703, 'learning_rate': 4.999740375012069e-05, 'epoch': 0.0}
{'loss': 8.7631, 'grad_norm': 5.848190784454346, 'learning_rate': 4.9997200918098865e-05, 'epoch': 0.0}
{'loss': 8.4029, 'grad_norm': 6.571547985076904, 'learning_rate': 4.9996998086077044e-05, 'epoch': 0.0}
{'loss': 8.5633, 'grad_norm': 5.594576835632324, 'learning_rate': 4.999679525405522e-05, 'epoch': 0.0}
{'loss': 8.388, 'grad_norm': 5.336413860321045, 'learning_rate': 4.99965924220334e-05, 'epoch': 0.0}
{'loss': 8.5745, 'grad_norm': 6.150896072387695, 'learning_rate': 4.999638959001158e-05, 'epoch': 0.0}
{'loss': 8.5293, 'grad_norm': 5.8571038246154785, 'learning_rate': 4.999618675798976e-05, 'epoch': 0.0}
{'loss': 9.152, 'grad_norm': 5.229250907897949, 'learning_rate': 4.9995983925967935e-05, 'epoch': 0.0}
{'loss': 8.3102, 'grad_norm': 5.358639717102051, 'learning_rate': 4.999578109394611e-05, 'epoch': 0.0}
{'loss': 8.426, 'grad_norm': 4.61784553527832, 'learning_rate': 4.999557826192429e-05, 'epoch': 0.0}
{'loss': 9.2006, 'grad_norm': 5.87870454788208, 'learning_rate': 4.9995375429902476e-05, 'epoch': 0.0}
{'loss': 8.0558, 'grad_norm': 5.161545753479004, 'learning_rate': 4.9995172597880654e-05, 'epoch': 0.0}
{'loss': 7.7619, 'grad_norm': 6.42789888381958, 'learning_rate': 4.999496976585883e-05, 'epoch': 0.0}
{'loss': 7.9367, 'grad_norm': 5.348819732666016, 'learning_rate': 4.9994766933837004e-05, 'epoch': 0.0}
{'loss': 7.7918, 'grad_norm': 5.677898406982422, 'learning_rate': 4.999456410181519e-05, 'epoch': 0.0}
{'loss': 7.8776, 'grad_norm': 7.284526824951172, 'learning_rate': 4.999436126979337e-05, 'epoch': 0.0}
{'loss': 8.1519, 'grad_norm': 5.147018909454346, 'learning_rate': 4.9994158437771545e-05, 'epoch': 0.0}
{'loss': 9.2294, 'grad_norm': 4.894822120666504, 'learning_rate': 4.999395560574972e-05, 'epoch': 0.0}
  File "c:\Users\omrid\Desktop\university\intelexsus\sandbox\sanskrit\model\train_model.py", line 62, in <module>00 [05:14<1094:31:43, 21.58s/it]
    main()
  File "c:\Users\omrid\Desktop\university\intelexsus\sandbox\sanskrit\model\train_model.py", line 46, in main
    run_training(
  File "c:\Users\omrid\Desktop\university\intelexsus\sandbox\sanskrit\model\training_utils.py", line 94, in run_training
    trainer.train()
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 2237, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 2660, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 3133, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 3082, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 4242, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 4427, in evaluation_loop
    for step, inputs in enumerate(dataloader):
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\accelerate\data_loader.py", line 577, in __iter__
    current_batch = send_to_device(current_batch, self.device, non_blocking=self._non_blocking)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\accelerate\utils\operations.py", line 153, in send_to_device
    return tensor.to(device, non_blocking=non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\tokenization_utils_base.py", line 809, in to
    self.data = {
                ^
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\tokenization_utils_base.py", line 810, in <dictcomp>
    k: v.to(device=device, non_blocking=non_blocking) if hasattr(v, "to") and callable(v.to) else v
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
