  0%|                                                                        | 0/200 [00:00<?, ?it/s]C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\torch\nn\modules\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
  1%|▋                                                               | 2/200 [00:01<01:38,  2.01it/s]C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\torch\nn\modules\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
{'loss': 10.5909, 'grad_norm': 25.463577270507812, 'learning_rate': 0.0, 'epoch': 0.5}
{'loss': 10.5938, 'grad_norm': 23.663890838623047, 'learning_rate': 8.333333333333334e-06, 'epoch': 1.0}
  return forward_call(*args, **kwargs)                                                               
{'eval_loss': 10.5625, 'eval_mlm_accuracy': 0.0, 'eval_runtime': 0.5054, 'eval_samples_per_second': 31.658, 'eval_steps_per_second': 3.957, 'epoch': 1.0}
  2%|█▎                                                              | 4/200 [00:04<02:48,  1.16it/s]C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\torch\nn\modules\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
{'loss': 10.375, 'grad_norm': 22.75802230834961, 'learning_rate': 1.6666666666666667e-05, 'epoch': 1.5}
{'loss': 10.3594, 'grad_norm': 20.910259246826172, 'learning_rate': 2.5e-05, 'epoch': 2.0}
  return forward_call(*args, **kwargs)                                                               
{'eval_loss': 10.465624809265137, 'eval_mlm_accuracy': 0.05714285714285714, 'eval_runtime': 0.5464, 'eval_samples_per_second': 29.283, 'eval_steps_per_second': 3.66, 'epoch': 2.0}
  3%|█▉                                                              | 6/200 [00:06<03:08,  1.03it/s]C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\torch\nn\modules\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
{'loss': 10.3281, 'grad_norm': 22.13409423828125, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.5}
{'loss': 10.3125, 'grad_norm': 21.395179748535156, 'learning_rate': 4.166666666666667e-05, 'epoch': 3.0}
  return forward_call(*args, **kwargs)                                                               
{'eval_loss': 10.331133842468262, 'eval_mlm_accuracy': 0.0, 'eval_runtime': 0.4605, 'eval_samples_per_second': 34.743, 'eval_steps_per_second': 4.343, 'epoch': 3.0}
  4%|██▌                                                             | 8/200 [00:08<03:09,  1.02it/s]C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\torch\nn\modules\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
{'loss': 10.1339, 'grad_norm': 19.76445770263672, 'learning_rate': 5e-05, 'epoch': 3.5}
{'loss': 9.5063, 'grad_norm': 13.633272171020508, 'learning_rate': 5.833333333333334e-05, 'epoch': 4.0}
  return forward_call(*args, **kwargs)                                                               
{'eval_loss': 10.334833145141602, 'eval_mlm_accuracy': 0.034482758620689655, 'eval_runtime': 0.4327, 'eval_samples_per_second': 36.978, 'eval_steps_per_second': 4.622, 'epoch': 4.0}
  5%|███▏                                                           | 10/200 [00:11<03:02,  1.04it/s]C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\torch\nn\modules\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
{'loss': 9.4236, 'grad_norm': 21.62725830078125, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.5}
{'loss': 9.446, 'grad_norm': 14.694802284240723, 'learning_rate': 7.500000000000001e-05, 'epoch': 5.0}
  return forward_call(*args, **kwargs)                                                               
{'eval_loss': 10.456730842590332, 'eval_mlm_accuracy': 0.0, 'eval_runtime': 0.4929, 'eval_samples_per_second': 32.463, 'eval_steps_per_second': 4.058, 'epoch': 5.0}
  6%|███▊                                                           | 12/200 [00:13<02:55,  1.07it/s]C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\torch\nn\modules\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
{'loss': 10.2102, 'grad_norm': 14.331888198852539, 'learning_rate': 8.333333333333334e-05, 'epoch': 5.5}
{'loss': 9.0045, 'grad_norm': 16.790977478027344, 'learning_rate': 9.166666666666667e-05, 'epoch': 6.0}
  return forward_call(*args, **kwargs)                                                               
{'eval_loss': 10.564582824707031, 'eval_mlm_accuracy': 0.0, 'eval_runtime': 0.4225, 'eval_samples_per_second': 37.869, 'eval_steps_per_second': 4.734, 'epoch': 6.0}
  7%|████▍                                                          | 14/200 [00:15<02:55,  1.06it/s]C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\torch\nn\modules\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
{'loss': 10.3341, 'grad_norm': 12.454460144042969, 'learning_rate': 0.0001, 'epoch': 6.5}
{'loss': 8.1901, 'grad_norm': 13.473821640014648, 'learning_rate': 9.946808510638298e-05, 'epoch': 7.0}
  return forward_call(*args, **kwargs)                                                               
{'eval_loss': 10.382401466369629, 'eval_mlm_accuracy': 0.04, 'eval_runtime': 0.4326, 'eval_samples_per_second': 36.982, 'eval_steps_per_second': 4.623, 'epoch': 7.0}
  8%|█████                                                          | 16/200 [00:17<02:48,  1.09it/s]Traceback (most recent call last):
{'loss': 9.5, 'grad_norm': 11.853726387023926, 'learning_rate': 9.893617021276596e-05, 'epoch': 7.5}
{'loss': 9.9193, 'grad_norm': 12.383878707885742, 'learning_rate': 9.840425531914894e-05, 'epoch': 8.0}
  File "c:\Users\omrid\Desktop\university\intelexsus\sandbox\sanskrit\model\debug_overfit.py", line 75, in <module>
{'eval_loss': 9.941570281982422, 'eval_mlm_accuracy': 0.0, 'eval_runtime': 0.4751, 'eval_samples_per_second': 33.678, 'eval_steps_per_second': 4.21, 'epoch': 8.0}
    main()
  File "c:\Users\omrid\Desktop\university\intelexsus\sandbox\sanskrit\model\debug_overfit.py", line 58, in main
    run_training(
  File "c:\Users\omrid\Desktop\university\intelexsus\sandbox\sanskrit\model\training_utils.py", line 98, in run_training
    trainer.train()
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 2237, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 2694, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 3140, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial)
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 3248, in _save_checkpoint
    self._save_optimizer_and_scheduler(output_dir)
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 3375, in _save_optimizer_and_scheduler
    torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\torch\serialization.py", line 850, in save
    _save(
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\torch\serialization.py", line 1114, in _save
    zip_file.write_record(name, storage, num_bytes)
KeyboardInterrupt
