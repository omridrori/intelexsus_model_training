  0%|                                                                                                                | 0/1825794 [00:00<?, ?it/s]C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\torch\nn\modules\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
  0%|                                                                                                  | 200/1825794 [02:25<205:47:11,  2.46it/s]Traceback (most recent call last):
{'loss': 10.3811, 'grad_norm': 18.983030319213867, 'learning_rate': 7e-08, 'epoch': 0.0}
{'loss': 11.4871, 'grad_norm': 17.62789535522461, 'learning_rate': 1.5000000000000002e-07, 'epoch': 0.0}
{'loss': 10.5461, 'grad_norm': 16.662242889404297, 'learning_rate': 2.3000000000000002e-07, 'epoch': 0.0}
{'loss': 15.9015, 'grad_norm': 14.809027671813965, 'learning_rate': 3.1000000000000005e-07, 'epoch': 0.0}
{'loss': 11.2412, 'grad_norm': 19.803861618041992, 'learning_rate': 3.9e-07, 'epoch': 0.0}
{'loss': 10.5042, 'grad_norm': 18.025239944458008, 'learning_rate': 4.7000000000000005e-07, 'epoch': 0.0}
{'loss': 12.511, 'grad_norm': 14.277852058410645, 'learning_rate': 5.5e-07, 'epoch': 0.0}
{'loss': 10.4291, 'grad_norm': 13.932964324951172, 'learning_rate': 6.3e-07, 'epoch': 0.0}
{'loss': 10.3738, 'grad_norm': 16.73758888244629, 'learning_rate': 7.1e-07, 'epoch': 0.0}
{'loss': 15.869, 'grad_norm': 17.98287582397461, 'learning_rate': 7.900000000000001e-07, 'epoch': 0.0}
{'loss': 13.6334, 'grad_norm': 11.534138679504395, 'learning_rate': 8.7e-07, 'epoch': 0.0}
{'loss': 10.3544, 'grad_norm': 16.20636749267578, 'learning_rate': 9.500000000000001e-07, 'epoch': 0.0}
{'loss': 10.2528, 'grad_norm': 15.045049667358398, 'learning_rate': 1.03e-06, 'epoch': 0.0}
{'loss': 12.1027, 'grad_norm': 13.014719009399414, 'learning_rate': 1.1100000000000002e-06, 'epoch': 0.0}
{'loss': 13.7078, 'grad_norm': 14.0425386428833, 'learning_rate': 1.19e-06, 'epoch': 0.0}
{'loss': 13.9297, 'grad_norm': 13.983798027038574, 'learning_rate': 1.2700000000000001e-06, 'epoch': 0.0}
{'loss': 11.7595, 'grad_norm': 12.348587036132812, 'learning_rate': 1.3500000000000002e-06, 'epoch': 0.0}
{'loss': 12.2816, 'grad_norm': 11.258442878723145, 'learning_rate': 1.43e-06, 'epoch': 0.0}
{'loss': 10.6827, 'grad_norm': 11.205790519714355, 'learning_rate': 1.5100000000000002e-06, 'epoch': 0.0}
{'loss': 11.1752, 'grad_norm': 15.344834327697754, 'learning_rate': 1.5900000000000002e-06, 'epoch': 0.0}
{'loss': 12.0989, 'grad_norm': 13.340909004211426, 'learning_rate': 1.6700000000000003e-06, 'epoch': 0.0}
{'loss': 11.9815, 'grad_norm': 10.274256706237793, 'learning_rate': 1.75e-06, 'epoch': 0.0}
{'loss': 10.1214, 'grad_norm': 11.197673797607422, 'learning_rate': 1.83e-06, 'epoch': 0.0}
{'loss': 12.3662, 'grad_norm': 12.815918922424316, 'learning_rate': 1.9100000000000003e-06, 'epoch': 0.0}
{'loss': 13.4492, 'grad_norm': 9.575369834899902, 'learning_rate': 1.9900000000000004e-06, 'epoch': 0.0}
  File "c:\Users\omrid\Desktop\university\intelexsus\sandbox\sanskrit\model\train_model.py", line 68, in <module> 93/800 [02:16<32:46,  2.78s/it]
    main()
  File "c:\Users\omrid\Desktop\university\intelexsus\sandbox\sanskrit\model\train_model.py", line 52, in main
    run_training(
  File "c:\Users\omrid\Desktop\university\intelexsus\sandbox\sanskrit\model\training_utils.py", line 93, in run_training
    trainer.train()
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 2237, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 2660, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 3133, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 3082, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 4242, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer.py", line 4490, in evaluation_loop
    all_labels.to_cpu_and_numpy()
  File "C:\Users\omrid\miniconda3\envs\INTELEXSUS\Lib\site-packages\transformers\trainer_pt_utils.py", line 321, in to_cpu_and_numpy
    def to_cpu_and_numpy(self) -> None:

KeyboardInterrupt
